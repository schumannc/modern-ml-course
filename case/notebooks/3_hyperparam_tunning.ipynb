{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/henriquecosta/workspace/studies/modern-ml'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "from sklearn.metrics import log_loss, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import optuna\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('./case/data/processed/lending_club_case_train_dataset_v2.parquet')\n",
    "df_valid = pd.read_parquet('./case/data/processed/lending_club_case_valid_dataset_v2.parquet')\n",
    "df_test  = pd.read_parquet('./case/data/processed/lending_club_case_test_dataset_v2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    'loan_amnt',\n",
    "    'term',\n",
    "    'int_rate',\n",
    "    'installment',\n",
    "    'annual_inc',\n",
    "    'dti',\n",
    "    'fico_range_low',\n",
    "    'inq_last_6mths',\n",
    "    'mths_since_last_delinq',\n",
    "    'revol_bal',\n",
    "    'total_rev_hi_lim',\n",
    "    'acc_open_past_24mths',\n",
    "    'bc_open_to_buy',\n",
    "    'bc_util',\n",
    "    'mo_sin_old_il_acct',\n",
    "    'mo_sin_old_rev_tl_op',\n",
    "    'mo_sin_rcnt_rev_tl_op',\n",
    "    'mo_sin_rcnt_tl',\n",
    "    'mort_acc',\n",
    "    'mths_since_recent_bc',\n",
    "    'mths_since_recent_inq',\n",
    "    'num_bc_tl',\n",
    "    'num_il_tl',\n",
    "    'num_rev_tl_bal_gt_0',\n",
    "    'percent_bc_gt_75',\n",
    "    'tot_hi_cred_lim',\n",
    "    'total_bc_limit',\n",
    "    'total_il_high_credit_limit',\n",
    "    'issue_d_elapse',\n",
    "    'earliest_cr_line_since',\n",
    "    'grade',\n",
    "    'sub_grade',\n",
    "    'emp_length',\n",
    "    'home_ownership',\n",
    "    'verification_status',\n",
    "    'pymnt_plan',\n",
    "    'purpose',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'addr_state',\n",
    "    'initial_list_status',\n",
    "    'application_type',\n",
    "    'verification_status_joint',\n",
    "    'issue_d_month',\n",
    "    'issue_d_day',\n",
    "    'issue_d_dayofweek',\n",
    "    'issue_d_dayofyear',\n",
    "    'issue_d_quarter',\n",
    "    'earliest_cr_line_month',\n",
    "    'earliest_cr_line_day',\n",
    "    'earliest_cr_line_dayofweek',\n",
    "    'earliest_cr_line_dayofyear',\n",
    "    'earliest_cr_line_quarter',\n",
    "    'sec_app_earliest_cr_line_month',\n",
    "    'sec_app_earliest_cr_line_day',\n",
    "    'sec_app_earliest_cr_line_dayofweek',\n",
    "    'sec_app_earliest_cr_line_dayofyear',\n",
    "    'sec_app_earliest_cr_line_quarter'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [c for c in df_train.select_dtypes(include='category').columns if c in FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[FEATURES]\n",
    "y_train = df_train['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_train, _, y_train = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/02 16:40:29 INFO mlflow.tracking.fluent: Experiment with name 'tunning_hyperparam' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/henriquecosta/workspace/studies/modern-ml/mlruns/222273069811250698', creation_time=1738525229143, experiment_id='222273069811250698', last_update_time=1738525229143, lifecycle_stage='active', name='tunning_hyperparam', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('tunning_hyperparam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = df_valid[FEATURES]\n",
    "y_valid = df_valid['default']\n",
    "\n",
    "train_pool = cb.Pool(X_train, y_train, cat_features=CATEGORICAL_FEATURES)\n",
    "valid_pool = cb.Pool(X_valid, y_valid, cat_features=CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.141198\n",
      "0:\tlearn: 0.6764790\ttest: 0.6767839\tbest: 0.6767839 (0)\ttotal: 720ms\tremaining: 11m 59s\n",
      "10:\tlearn: 0.6260489\ttest: 0.6335844\tbest: 0.6335844 (10)\ttotal: 5.93s\tremaining: 8m 52s\n",
      "20:\tlearn: 0.6187232\ttest: 0.6274173\tbest: 0.6274173 (20)\ttotal: 10.7s\tremaining: 8m 18s\n",
      "30:\tlearn: 0.6156667\ttest: 0.6235768\tbest: 0.6235768 (30)\ttotal: 15.3s\tremaining: 7m 59s\n",
      "40:\tlearn: 0.6138170\ttest: 0.6218669\tbest: 0.6218669 (40)\ttotal: 19.9s\tremaining: 7m 46s\n",
      "50:\tlearn: 0.6127659\ttest: 0.6212987\tbest: 0.6212987 (50)\ttotal: 25.3s\tremaining: 7m 50s\n",
      "60:\tlearn: 0.6117360\ttest: 0.6204772\tbest: 0.6204772 (60)\ttotal: 30.7s\tremaining: 7m 52s\n",
      "70:\tlearn: 0.6109311\ttest: 0.6202244\tbest: 0.6200282 (63)\ttotal: 36s\tremaining: 7m 51s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.6200282114\n",
      "bestIteration = 63\n",
      "\n",
      "Shrink model to first 64 iterations.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='vanilla-Catboost'):\n",
    "    params = dict(\n",
    "        iterations=1000,\n",
    "        depth=6,\n",
    "        auto_class_weights='Balanced',\n",
    "        eval_metric='Logloss',\n",
    "        verbose=10,\n",
    "    )\n",
    "\n",
    "    model = cb.CatBoostClassifier(**params)\n",
    "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=10)\n",
    "    y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric('log_loss', log_loss(y_valid, y_pred))\n",
    "    mlflow.log_metric('avg_pr', average_precision_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        params = dict(\n",
    "            iterations=500,\n",
    "            depth=trial.suggest_int('depth', 4, 10),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "            l2_leaf_reg=trial.suggest_int('l2_leaf_reg', 1, 20),\n",
    "            colsample_bylevel=trial.suggest_float('colsample_bylevel', 0.7, 1.0, step=0.1),\n",
    "            subsample=trial.suggest_float('colsample_bylevel', 0.5, 1.0, step=0.1),\n",
    "            eval_metric='Logloss',  \n",
    "        )\n",
    "\n",
    "        model = cb.CatBoostClassifier(**params)\n",
    "        \n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for train_index, valid_index in kf.split(X_train):\n",
    "            X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "            y_tr, y_val = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "            train_pool = cb.Pool(X_tr, y_tr, cat_features=CATEGORICAL_FEATURES)\n",
    "            val_pool = cb.Pool(X_val, y_val, cat_features=CATEGORICAL_FEATURES)\n",
    "\n",
    "            model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=10, verbose=False)\n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "            score = log_loss(y_val, y_pred)\n",
    "            \n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        mlflow.log_metric('log_loss', avg_score)\n",
    "        mlflow.log_params(params)\n",
    "    return np.mean(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n",
      "/Users/henriquecosta/workspace/studies/modern-ml/.venv/lib/python3.11/site-packages/optuna/trial/_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"colsample_bylevel\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': 0.1, 'low': 0.7, 'high': 1.0, 'log': False}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6421645\ttest: 0.6487584\tbest: 0.6487584 (0)\ttotal: 3.47s\tremaining: 57m 47s\n",
      "1:\tlearn: 0.6007537\ttest: 0.6119180\tbest: 0.6119180 (1)\ttotal: 10.2s\tremaining: 1h 24m 34s\n",
      "2:\tlearn: 0.5678102\ttest: 0.5851345\tbest: 0.5851345 (2)\ttotal: 22.3s\tremaining: 2h 3m 24s\n",
      "3:\tlearn: 0.5425788\ttest: 0.5646542\tbest: 0.5646542 (3)\ttotal: 25.7s\tremaining: 1h 46m 46s\n",
      "4:\tlearn: 0.5215553\ttest: 0.5486170\tbest: 0.5486170 (4)\ttotal: 30.8s\tremaining: 1h 42m 4s\n",
      "5:\tlearn: 0.5055012\ttest: 0.5374531\tbest: 0.5374531 (5)\ttotal: 48.6s\tremaining: 2h 14m 7s\n",
      "6:\tlearn: 0.4928793\ttest: 0.5288366\tbest: 0.5288366 (6)\ttotal: 57s\tremaining: 2h 14m 46s\n",
      "7:\tlearn: 0.4824322\ttest: 0.5214940\tbest: 0.5214940 (7)\ttotal: 1m 3s\tremaining: 2h 11m 44s\n",
      "8:\tlearn: 0.4738273\ttest: 0.5157163\tbest: 0.5157163 (8)\ttotal: 1m 11s\tremaining: 2h 11m 54s\n",
      "9:\tlearn: 0.4667372\ttest: 0.5117830\tbest: 0.5117830 (9)\ttotal: 1m 17s\tremaining: 2h 7m 44s\n",
      "10:\tlearn: 0.4614421\ttest: 0.5090383\tbest: 0.5090383 (10)\ttotal: 1m 23s\tremaining: 2h 5m 14s\n",
      "11:\tlearn: 0.4565960\ttest: 0.5064678\tbest: 0.5064678 (11)\ttotal: 1m 30s\tremaining: 2h 3m 46s\n",
      "12:\tlearn: 0.4530117\ttest: 0.5050515\tbest: 0.5050515 (12)\ttotal: 1m 33s\tremaining: 1h 58m 34s\n",
      "13:\tlearn: 0.4502766\ttest: 0.5041308\tbest: 0.5041308 (13)\ttotal: 1m 45s\tremaining: 2h 4m 13s\n",
      "14:\tlearn: 0.4479996\ttest: 0.5035649\tbest: 0.5035649 (14)\ttotal: 1m 54s\tremaining: 2h 5m 27s\n",
      "15:\tlearn: 0.4458156\ttest: 0.5027546\tbest: 0.5027546 (15)\ttotal: 2m 10s\tremaining: 2h 13m 33s\n",
      "16:\tlearn: 0.4437656\ttest: 0.5019659\tbest: 0.5019659 (16)\ttotal: 2m 23s\tremaining: 2h 18m 23s\n",
      "17:\tlearn: 0.4420120\ttest: 0.5015090\tbest: 0.5015090 (17)\ttotal: 2m 29s\tremaining: 2h 16m 22s\n",
      "18:\tlearn: 0.4410224\ttest: 0.5015934\tbest: 0.5015090 (17)\ttotal: 2m 35s\tremaining: 2h 13m 24s\n",
      "19:\tlearn: 0.4397534\ttest: 0.5013002\tbest: 0.5013002 (19)\ttotal: 2m 41s\tremaining: 2h 11m 50s\n",
      "20:\tlearn: 0.4387807\ttest: 0.5010169\tbest: 0.5010169 (20)\ttotal: 2m 52s\tremaining: 2h 13m 58s\n",
      "21:\tlearn: 0.4378563\ttest: 0.5010156\tbest: 0.5010156 (21)\ttotal: 2m 55s\tremaining: 2h 10m 20s\n",
      "22:\tlearn: 0.4370806\ttest: 0.5008668\tbest: 0.5008668 (22)\ttotal: 2m 59s\tremaining: 2h 7m 3s\n",
      "23:\tlearn: 0.4364019\ttest: 0.5008574\tbest: 0.5008574 (23)\ttotal: 3m 12s\tremaining: 2h 10m 32s\n",
      "24:\tlearn: 0.4357582\ttest: 0.5007373\tbest: 0.5007373 (24)\ttotal: 3m 28s\tremaining: 2h 15m 25s\n",
      "25:\tlearn: 0.4352638\ttest: 0.5008439\tbest: 0.5007373 (24)\ttotal: 3m 38s\tremaining: 2h 16m 15s\n",
      "26:\tlearn: 0.4347928\ttest: 0.5008982\tbest: 0.5007373 (24)\ttotal: 3m 44s\tremaining: 2h 14m 48s\n",
      "27:\tlearn: 0.4343554\ttest: 0.5007632\tbest: 0.5007373 (24)\ttotal: 3m 47s\tremaining: 2h 11m 39s\n",
      "28:\tlearn: 0.4339222\ttest: 0.5007287\tbest: 0.5007287 (28)\ttotal: 3m 49s\tremaining: 2h 8m 12s\n",
      "29:\tlearn: 0.4335735\ttest: 0.5008600\tbest: 0.5007287 (28)\ttotal: 3m 52s\tremaining: 2h 5m 29s\n",
      "30:\tlearn: 0.4332331\ttest: 0.5008157\tbest: 0.5007287 (28)\ttotal: 3m 58s\tremaining: 2h 4m 21s\n",
      "31:\tlearn: 0.4328891\ttest: 0.5007169\tbest: 0.5007169 (31)\ttotal: 4m 2s\tremaining: 2h 2m 13s\n",
      "32:\tlearn: 0.4326431\ttest: 0.5007727\tbest: 0.5007169 (31)\ttotal: 4m 5s\tremaining: 2h 1s\n",
      "33:\tlearn: 0.4323548\ttest: 0.5007247\tbest: 0.5007169 (31)\ttotal: 4m 9s\tremaining: 1h 57m 54s\n",
      "34:\tlearn: 0.4321211\ttest: 0.5006385\tbest: 0.5006385 (34)\ttotal: 4m 16s\tremaining: 1h 57m 49s\n",
      "35:\tlearn: 0.4318875\ttest: 0.5005748\tbest: 0.5005748 (35)\ttotal: 4m 18s\tremaining: 1h 55m 29s\n",
      "36:\tlearn: 0.4316555\ttest: 0.5003969\tbest: 0.5003969 (36)\ttotal: 4m 20s\tremaining: 1h 52m 50s\n",
      "37:\tlearn: 0.4315181\ttest: 0.5003772\tbest: 0.5003772 (37)\ttotal: 4m 22s\tremaining: 1h 50m 50s\n",
      "38:\tlearn: 0.4313603\ttest: 0.5004031\tbest: 0.5003772 (37)\ttotal: 4m 25s\tremaining: 1h 48m 53s\n",
      "39:\tlearn: 0.4312159\ttest: 0.5004243\tbest: 0.5003772 (37)\ttotal: 4m 28s\tremaining: 1h 47m 14s\n",
      "40:\tlearn: 0.4310543\ttest: 0.5003942\tbest: 0.5003772 (37)\ttotal: 4m 31s\tremaining: 1h 45m 42s\n",
      "41:\tlearn: 0.4308424\ttest: 0.5002784\tbest: 0.5002784 (41)\ttotal: 4m 33s\tremaining: 1h 44m 2s\n",
      "42:\tlearn: 0.4306647\ttest: 0.5002158\tbest: 0.5002158 (42)\ttotal: 4m 36s\tremaining: 1h 42m 37s\n",
      "43:\tlearn: 0.4304902\ttest: 0.5001339\tbest: 0.5001339 (43)\ttotal: 4m 38s\tremaining: 1h 40m 52s\n",
      "44:\tlearn: 0.4303567\ttest: 0.5001501\tbest: 0.5001339 (43)\ttotal: 4m 39s\tremaining: 1h 38m 55s\n",
      "45:\tlearn: 0.4302072\ttest: 0.5000076\tbest: 0.5000076 (45)\ttotal: 4m 40s\tremaining: 1h 37m 5s\n",
      "46:\tlearn: 0.4300523\ttest: 0.4998695\tbest: 0.4998695 (46)\ttotal: 4m 44s\tremaining: 1h 36m 13s\n",
      "47:\tlearn: 0.4299276\ttest: 0.4998780\tbest: 0.4998695 (46)\ttotal: 4m 47s\tremaining: 1h 34m 55s\n",
      "48:\tlearn: 0.4298403\ttest: 0.4998370\tbest: 0.4998370 (48)\ttotal: 4m 50s\tremaining: 1h 33m 48s\n",
      "49:\tlearn: 0.4296982\ttest: 0.4996693\tbest: 0.4996693 (49)\ttotal: 4m 51s\tremaining: 1h 32m 27s\n",
      "50:\tlearn: 0.4295989\ttest: 0.4996336\tbest: 0.4996336 (50)\ttotal: 4m 53s\tremaining: 1h 30m 53s\n",
      "51:\tlearn: 0.4294852\ttest: 0.4995827\tbest: 0.4995827 (51)\ttotal: 4m 54s\tremaining: 1h 29m 23s\n",
      "52:\tlearn: 0.4293582\ttest: 0.4994434\tbest: 0.4994434 (52)\ttotal: 4m 55s\tremaining: 1h 27m 57s\n",
      "53:\tlearn: 0.4292867\ttest: 0.4994526\tbest: 0.4994434 (52)\ttotal: 4m 56s\tremaining: 1h 26m 35s\n",
      "54:\tlearn: 0.4291786\ttest: 0.4993754\tbest: 0.4993754 (54)\ttotal: 4m 57s\tremaining: 1h 25m 14s\n",
      "55:\tlearn: 0.4290885\ttest: 0.4993398\tbest: 0.4993398 (55)\ttotal: 4m 58s\tremaining: 1h 23m 56s\n",
      "56:\tlearn: 0.4289645\ttest: 0.4992146\tbest: 0.4992146 (56)\ttotal: 4m 59s\tremaining: 1h 22m 37s\n",
      "57:\tlearn: 0.4288715\ttest: 0.4991375\tbest: 0.4991375 (57)\ttotal: 5m\tremaining: 1h 21m 23s\n",
      "58:\tlearn: 0.4287722\ttest: 0.4990607\tbest: 0.4990607 (58)\ttotal: 5m 1s\tremaining: 1h 20m 11s\n",
      "59:\tlearn: 0.4286697\ttest: 0.4990086\tbest: 0.4990086 (59)\ttotal: 5m 2s\tremaining: 1h 18m 59s\n",
      "60:\tlearn: 0.4285644\ttest: 0.4988717\tbest: 0.4988717 (60)\ttotal: 5m 3s\tremaining: 1h 17m 49s\n",
      "61:\tlearn: 0.4284810\ttest: 0.4987817\tbest: 0.4987817 (61)\ttotal: 5m 5s\tremaining: 1h 17m 3s\n",
      "62:\tlearn: 0.4283901\ttest: 0.4987267\tbest: 0.4987267 (62)\ttotal: 5m 6s\tremaining: 1h 15m 59s\n",
      "63:\tlearn: 0.4283008\ttest: 0.4987824\tbest: 0.4987267 (62)\ttotal: 5m 7s\tremaining: 1h 14m 57s\n",
      "64:\tlearn: 0.4282074\ttest: 0.4987601\tbest: 0.4987267 (62)\ttotal: 5m 8s\tremaining: 1h 13m 54s\n",
      "65:\tlearn: 0.4281485\ttest: 0.4987397\tbest: 0.4987267 (62)\ttotal: 5m 9s\tremaining: 1h 12m 57s\n",
      "66:\tlearn: 0.4280244\ttest: 0.4987651\tbest: 0.4987267 (62)\ttotal: 5m 10s\tremaining: 1h 11m 58s\n",
      "67:\tlearn: 0.4279173\ttest: 0.4986726\tbest: 0.4986726 (67)\ttotal: 5m 10s\tremaining: 1h 11m 1s\n",
      "68:\tlearn: 0.4278268\ttest: 0.4986198\tbest: 0.4986198 (68)\ttotal: 5m 11s\tremaining: 1h 10m 8s\n",
      "69:\tlearn: 0.4277366\ttest: 0.4985497\tbest: 0.4985497 (69)\ttotal: 5m 12s\tremaining: 1h 9m 13s\n",
      "70:\tlearn: 0.4276800\ttest: 0.4985248\tbest: 0.4985248 (70)\ttotal: 5m 13s\tremaining: 1h 8m 26s\n",
      "71:\tlearn: 0.4276371\ttest: 0.4985436\tbest: 0.4985248 (70)\ttotal: 5m 20s\tremaining: 1h 8m 49s\n",
      "72:\tlearn: 0.4275693\ttest: 0.4986193\tbest: 0.4985248 (70)\ttotal: 5m 23s\tremaining: 1h 8m 23s\n",
      "73:\tlearn: 0.4275058\ttest: 0.4984966\tbest: 0.4984966 (73)\ttotal: 5m 24s\tremaining: 1h 7m 42s\n",
      "74:\tlearn: 0.4274333\ttest: 0.4984655\tbest: 0.4984655 (74)\ttotal: 5m 25s\tremaining: 1h 7m\n",
      "75:\tlearn: 0.4273914\ttest: 0.4984651\tbest: 0.4984651 (75)\ttotal: 5m 27s\tremaining: 1h 6m 17s\n",
      "76:\tlearn: 0.4273123\ttest: 0.4985438\tbest: 0.4984651 (75)\ttotal: 5m 28s\tremaining: 1h 5m 32s\n",
      "77:\tlearn: 0.4272390\ttest: 0.4985514\tbest: 0.4984651 (75)\ttotal: 5m 28s\tremaining: 1h 4m 48s\n",
      "78:\tlearn: 0.4271665\ttest: 0.4985684\tbest: 0.4984651 (75)\ttotal: 5m 33s\tremaining: 1h 4m 49s\n",
      "79:\tlearn: 0.4271048\ttest: 0.4986010\tbest: 0.4984651 (75)\ttotal: 5m 36s\tremaining: 1h 4m 34s\n",
      "80:\tlearn: 0.4270716\ttest: 0.4985848\tbest: 0.4984651 (75)\ttotal: 5m 38s\tremaining: 1h 4m 4s\n",
      "81:\tlearn: 0.4270048\ttest: 0.4985433\tbest: 0.4984651 (75)\ttotal: 5m 40s\tremaining: 1h 3m 35s\n",
      "82:\tlearn: 0.4269188\ttest: 0.4984808\tbest: 0.4984651 (75)\ttotal: 5m 41s\tremaining: 1h 2m 58s\n",
      "83:\tlearn: 0.4268426\ttest: 0.4983831\tbest: 0.4983831 (83)\ttotal: 5m 42s\tremaining: 1h 2m 20s\n",
      "84:\tlearn: 0.4267990\ttest: 0.4985029\tbest: 0.4983831 (83)\ttotal: 5m 44s\tremaining: 1h 1m 44s\n",
      "85:\tlearn: 0.4267489\ttest: 0.4984372\tbest: 0.4983831 (83)\ttotal: 5m 45s\tremaining: 1h 1m 16s\n",
      "86:\tlearn: 0.4266990\ttest: 0.4984209\tbest: 0.4983831 (83)\ttotal: 5m 46s\tremaining: 1h 41s\n",
      "87:\tlearn: 0.4266626\ttest: 0.4984146\tbest: 0.4983831 (83)\ttotal: 5m 48s\tremaining: 1h 7s\n",
      "88:\tlearn: 0.4265971\ttest: 0.4984077\tbest: 0.4983831 (83)\ttotal: 5m 49s\tremaining: 59m 37s\n",
      "89:\tlearn: 0.4265405\ttest: 0.4983536\tbest: 0.4983536 (89)\ttotal: 5m 51s\tremaining: 59m 14s\n",
      "90:\tlearn: 0.4264422\ttest: 0.4982767\tbest: 0.4982767 (90)\ttotal: 5m 52s\tremaining: 58m 41s\n",
      "91:\tlearn: 0.4264012\ttest: 0.4982279\tbest: 0.4982279 (91)\ttotal: 5m 56s\tremaining: 58m 35s\n",
      "92:\tlearn: 0.4263371\ttest: 0.4982008\tbest: 0.4982008 (92)\ttotal: 5m 58s\tremaining: 58m 17s\n",
      "93:\tlearn: 0.4262674\ttest: 0.4982759\tbest: 0.4982008 (92)\ttotal: 5m 59s\tremaining: 57m 47s\n",
      "94:\tlearn: 0.4262146\ttest: 0.4982469\tbest: 0.4982008 (92)\ttotal: 6m\tremaining: 57m 16s\n",
      "95:\tlearn: 0.4261358\ttest: 0.4982288\tbest: 0.4982008 (92)\ttotal: 6m 1s\tremaining: 56m 46s\n",
      "96:\tlearn: 0.4260741\ttest: 0.4981639\tbest: 0.4981639 (96)\ttotal: 6m 2s\tremaining: 56m 17s\n",
      "97:\tlearn: 0.4259763\ttest: 0.4981375\tbest: 0.4981375 (97)\ttotal: 6m 3s\tremaining: 55m 46s\n",
      "98:\tlearn: 0.4258864\ttest: 0.4980938\tbest: 0.4980938 (98)\ttotal: 6m 4s\tremaining: 55m 18s\n",
      "99:\tlearn: 0.4258298\ttest: 0.4980724\tbest: 0.4980724 (99)\ttotal: 6m 5s\tremaining: 54m 52s\n",
      "100:\tlearn: 0.4257685\ttest: 0.4980235\tbest: 0.4980235 (100)\ttotal: 6m 6s\tremaining: 54m 25s\n",
      "101:\tlearn: 0.4257127\ttest: 0.4980034\tbest: 0.4980034 (101)\ttotal: 6m 7s\tremaining: 53m 58s\n",
      "102:\tlearn: 0.4256622\ttest: 0.4979710\tbest: 0.4979710 (102)\ttotal: 6m 8s\tremaining: 53m 32s\n",
      "103:\tlearn: 0.4256368\ttest: 0.4979966\tbest: 0.4979710 (102)\ttotal: 6m 10s\tremaining: 53m 12s\n",
      "104:\tlearn: 0.4255420\ttest: 0.4978888\tbest: 0.4978888 (104)\ttotal: 6m 11s\tremaining: 52m 46s\n",
      "105:\tlearn: 0.4254885\ttest: 0.4978052\tbest: 0.4978052 (105)\ttotal: 6m 12s\tremaining: 52m 21s\n",
      "106:\tlearn: 0.4253930\ttest: 0.4977248\tbest: 0.4977248 (106)\ttotal: 6m 13s\tremaining: 51m 55s\n",
      "107:\tlearn: 0.4253065\ttest: 0.4977293\tbest: 0.4977248 (106)\ttotal: 6m 14s\tremaining: 51m 29s\n",
      "108:\tlearn: 0.4252691\ttest: 0.4977320\tbest: 0.4977248 (106)\ttotal: 6m 15s\tremaining: 51m 6s\n",
      "109:\tlearn: 0.4252335\ttest: 0.4976986\tbest: 0.4976986 (109)\ttotal: 6m 17s\tremaining: 50m 57s\n",
      "110:\tlearn: 0.4251927\ttest: 0.4976529\tbest: 0.4976529 (110)\ttotal: 6m 18s\tremaining: 50m 34s\n",
      "111:\tlearn: 0.4251297\ttest: 0.4976421\tbest: 0.4976421 (111)\ttotal: 6m 20s\tremaining: 50m 14s\n",
      "112:\tlearn: 0.4250668\ttest: 0.4975917\tbest: 0.4975917 (112)\ttotal: 6m 27s\tremaining: 50m 43s\n",
      "113:\tlearn: 0.4250057\ttest: 0.4975617\tbest: 0.4975617 (113)\ttotal: 6m 31s\tremaining: 50m 40s\n",
      "114:\tlearn: 0.4249457\ttest: 0.4975478\tbest: 0.4975478 (114)\ttotal: 6m 33s\tremaining: 50m 29s\n",
      "115:\tlearn: 0.4248872\ttest: 0.4975568\tbest: 0.4975478 (114)\ttotal: 6m 36s\tremaining: 50m 19s\n",
      "116:\tlearn: 0.4248803\ttest: 0.4975510\tbest: 0.4975478 (114)\ttotal: 6m 36s\tremaining: 49m 52s\n",
      "117:\tlearn: 0.4248253\ttest: 0.4975639\tbest: 0.4975478 (114)\ttotal: 6m 41s\tremaining: 50m 2s\n",
      "118:\tlearn: 0.4247681\ttest: 0.4975657\tbest: 0.4975478 (114)\ttotal: 6m 49s\tremaining: 50m 29s\n",
      "119:\tlearn: 0.4247450\ttest: 0.4975407\tbest: 0.4975407 (119)\ttotal: 6m 53s\tremaining: 50m 33s\n",
      "120:\tlearn: 0.4246897\ttest: 0.4975369\tbest: 0.4975369 (120)\ttotal: 6m 59s\tremaining: 50m 46s\n",
      "121:\tlearn: 0.4246323\ttest: 0.4975399\tbest: 0.4975369 (120)\ttotal: 7m 1s\tremaining: 50m 36s\n",
      "122:\tlearn: 0.4245690\ttest: 0.4975296\tbest: 0.4975296 (122)\ttotal: 7m 3s\tremaining: 50m 18s\n",
      "123:\tlearn: 0.4244847\ttest: 0.4975203\tbest: 0.4975203 (123)\ttotal: 7m 4s\tremaining: 50m\n",
      "124:\tlearn: 0.4244238\ttest: 0.4974055\tbest: 0.4974055 (124)\ttotal: 7m 6s\tremaining: 49m 47s\n",
      "125:\tlearn: 0.4243441\ttest: 0.4973768\tbest: 0.4973768 (125)\ttotal: 7m 7s\tremaining: 49m 28s\n",
      "126:\tlearn: 0.4242921\ttest: 0.4973211\tbest: 0.4973211 (126)\ttotal: 7m 9s\tremaining: 49m 14s\n",
      "127:\tlearn: 0.4242204\ttest: 0.4972899\tbest: 0.4972899 (127)\ttotal: 7m 12s\tremaining: 49m 6s\n",
      "128:\tlearn: 0.4241721\ttest: 0.4972522\tbest: 0.4972522 (128)\ttotal: 7m 20s\tremaining: 49m 34s\n",
      "129:\tlearn: 0.4241109\ttest: 0.4972641\tbest: 0.4972522 (128)\ttotal: 7m 28s\tremaining: 50m\n",
      "130:\tlearn: 0.4240490\ttest: 0.4972535\tbest: 0.4972522 (128)\ttotal: 7m 38s\tremaining: 50m 42s\n",
      "131:\tlearn: 0.4239632\ttest: 0.4972141\tbest: 0.4972141 (131)\ttotal: 7m 44s\tremaining: 50m 54s\n",
      "132:\tlearn: 0.4238833\ttest: 0.4971710\tbest: 0.4971710 (132)\ttotal: 7m 49s\tremaining: 50m 58s\n",
      "133:\tlearn: 0.4238418\ttest: 0.4971069\tbest: 0.4971069 (133)\ttotal: 7m 54s\tremaining: 51m 4s\n",
      "134:\tlearn: 0.4237759\ttest: 0.4971444\tbest: 0.4971069 (133)\ttotal: 7m 59s\tremaining: 51m 14s\n",
      "135:\tlearn: 0.4237228\ttest: 0.4971202\tbest: 0.4971069 (133)\ttotal: 8m 3s\tremaining: 51m 12s\n",
      "136:\tlearn: 0.4236452\ttest: 0.4971214\tbest: 0.4971069 (133)\ttotal: 8m 6s\tremaining: 51m 2s\n",
      "137:\tlearn: 0.4235818\ttest: 0.4971197\tbest: 0.4971069 (133)\ttotal: 8m 9s\tremaining: 50m 55s\n",
      "138:\tlearn: 0.4234886\ttest: 0.4970968\tbest: 0.4970968 (138)\ttotal: 8m 12s\tremaining: 50m 49s\n",
      "139:\tlearn: 0.4234041\ttest: 0.4970888\tbest: 0.4970888 (139)\ttotal: 8m 14s\tremaining: 50m 35s\n",
      "140:\tlearn: 0.4233571\ttest: 0.4970684\tbest: 0.4970684 (140)\ttotal: 8m 15s\tremaining: 50m 21s\n",
      "141:\tlearn: 0.4232532\ttest: 0.4970180\tbest: 0.4970180 (141)\ttotal: 8m 17s\tremaining: 50m 3s\n",
      "142:\tlearn: 0.4231744\ttest: 0.4970071\tbest: 0.4970071 (142)\ttotal: 8m 18s\tremaining: 49m 46s\n",
      "143:\tlearn: 0.4230959\ttest: 0.4969374\tbest: 0.4969374 (143)\ttotal: 8m 19s\tremaining: 49m 27s\n",
      "144:\tlearn: 0.4230053\ttest: 0.4968541\tbest: 0.4968541 (144)\ttotal: 8m 20s\tremaining: 49m 11s\n",
      "145:\tlearn: 0.4229474\ttest: 0.4968234\tbest: 0.4968234 (145)\ttotal: 8m 21s\tremaining: 48m 54s\n",
      "146:\tlearn: 0.4229001\ttest: 0.4968174\tbest: 0.4968174 (146)\ttotal: 8m 26s\tremaining: 48m 59s\n",
      "147:\tlearn: 0.4228284\ttest: 0.4965848\tbest: 0.4965848 (147)\ttotal: 8m 29s\tremaining: 48m 53s\n",
      "148:\tlearn: 0.4227790\ttest: 0.4966692\tbest: 0.4965848 (147)\ttotal: 8m 30s\tremaining: 48m 37s\n",
      "149:\tlearn: 0.4227121\ttest: 0.4966470\tbest: 0.4965848 (147)\ttotal: 8m 31s\tremaining: 48m 20s\n",
      "150:\tlearn: 0.4226786\ttest: 0.4966164\tbest: 0.4965848 (147)\ttotal: 8m 32s\tremaining: 48m 3s\n",
      "151:\tlearn: 0.4226216\ttest: 0.4965863\tbest: 0.4965848 (147)\ttotal: 8m 35s\tremaining: 47m 53s\n",
      "152:\tlearn: 0.4225786\ttest: 0.4965834\tbest: 0.4965834 (152)\ttotal: 8m 39s\tremaining: 47m 53s\n",
      "153:\tlearn: 0.4225222\ttest: 0.4965809\tbest: 0.4965809 (153)\ttotal: 8m 40s\tremaining: 47m 38s\n",
      "154:\tlearn: 0.4224242\ttest: 0.4965668\tbest: 0.4965668 (154)\ttotal: 8m 41s\tremaining: 47m 25s\n",
      "155:\tlearn: 0.4223652\ttest: 0.4966051\tbest: 0.4965668 (154)\ttotal: 8m 43s\tremaining: 47m 11s\n",
      "156:\tlearn: 0.4223126\ttest: 0.4965566\tbest: 0.4965566 (156)\ttotal: 8m 45s\tremaining: 47m\n",
      "157:\tlearn: 0.4222560\ttest: 0.4963336\tbest: 0.4963336 (157)\ttotal: 8m 46s\tremaining: 46m 46s\n",
      "158:\tlearn: 0.4221662\ttest: 0.4962990\tbest: 0.4962990 (158)\ttotal: 8m 47s\tremaining: 46m 31s\n",
      "159:\tlearn: 0.4220696\ttest: 0.4962673\tbest: 0.4962673 (159)\ttotal: 8m 48s\tremaining: 46m 14s\n",
      "160:\tlearn: 0.4219898\ttest: 0.4963284\tbest: 0.4962673 (159)\ttotal: 8m 49s\tremaining: 45m 58s\n",
      "161:\tlearn: 0.4219041\ttest: 0.4963101\tbest: 0.4962673 (159)\ttotal: 8m 50s\tremaining: 45m 42s\n",
      "162:\tlearn: 0.4218419\ttest: 0.4962662\tbest: 0.4962662 (162)\ttotal: 8m 50s\tremaining: 45m 26s\n",
      "163:\tlearn: 0.4217595\ttest: 0.4962724\tbest: 0.4962662 (162)\ttotal: 8m 51s\tremaining: 45m 10s\n",
      "164:\tlearn: 0.4217114\ttest: 0.4962537\tbest: 0.4962537 (164)\ttotal: 8m 52s\tremaining: 44m 55s\n",
      "165:\tlearn: 0.4216501\ttest: 0.4962376\tbest: 0.4962376 (165)\ttotal: 8m 53s\tremaining: 44m 42s\n",
      "166:\tlearn: 0.4215536\ttest: 0.4961667\tbest: 0.4961667 (166)\ttotal: 8m 54s\tremaining: 44m 27s\n",
      "167:\tlearn: 0.4215126\ttest: 0.4961586\tbest: 0.4961586 (167)\ttotal: 8m 56s\tremaining: 44m 17s\n",
      "168:\tlearn: 0.4214756\ttest: 0.4958345\tbest: 0.4958345 (168)\ttotal: 9m\tremaining: 44m 15s\n",
      "169:\tlearn: 0.4214152\ttest: 0.4957903\tbest: 0.4957903 (169)\ttotal: 9m 1s\tremaining: 44m 5s\n",
      "170:\tlearn: 0.4213148\ttest: 0.4957304\tbest: 0.4957304 (170)\ttotal: 9m 3s\tremaining: 43m 53s\n",
      "171:\tlearn: 0.4212702\ttest: 0.4956745\tbest: 0.4956745 (171)\ttotal: 9m 4s\tremaining: 43m 40s\n",
      "172:\tlearn: 0.4211861\ttest: 0.4956217\tbest: 0.4956217 (172)\ttotal: 9m 5s\tremaining: 43m 26s\n",
      "173:\tlearn: 0.4211248\ttest: 0.4956184\tbest: 0.4956184 (173)\ttotal: 9m 6s\tremaining: 43m 12s\n",
      "174:\tlearn: 0.4210912\ttest: 0.4955919\tbest: 0.4955919 (174)\ttotal: 9m 7s\tremaining: 42m 58s\n",
      "175:\tlearn: 0.4210328\ttest: 0.4956072\tbest: 0.4955919 (174)\ttotal: 9m 7s\tremaining: 42m 44s\n",
      "176:\tlearn: 0.4209659\ttest: 0.4957180\tbest: 0.4955919 (174)\ttotal: 9m 8s\tremaining: 42m 31s\n",
      "177:\tlearn: 0.4209281\ttest: 0.4956531\tbest: 0.4955919 (174)\ttotal: 9m 9s\tremaining: 42m 19s\n",
      "178:\tlearn: 0.4208428\ttest: 0.4956436\tbest: 0.4955919 (174)\ttotal: 9m 10s\tremaining: 42m 6s\n",
      "179:\tlearn: 0.4207475\ttest: 0.4955793\tbest: 0.4955793 (179)\ttotal: 9m 12s\tremaining: 41m 55s\n",
      "180:\tlearn: 0.4206796\ttest: 0.4955510\tbest: 0.4955510 (180)\ttotal: 9m 14s\tremaining: 41m 47s\n",
      "181:\tlearn: 0.4206302\ttest: 0.4955682\tbest: 0.4955510 (180)\ttotal: 9m 15s\tremaining: 41m 38s\n",
      "182:\tlearn: 0.4205745\ttest: 0.4953164\tbest: 0.4953164 (182)\ttotal: 9m 17s\tremaining: 41m 27s\n",
      "183:\tlearn: 0.4204823\ttest: 0.4952962\tbest: 0.4952962 (183)\ttotal: 9m 18s\tremaining: 41m 15s\n",
      "184:\tlearn: 0.4204078\ttest: 0.4953022\tbest: 0.4952962 (183)\ttotal: 9m 19s\tremaining: 41m 4s\n",
      "185:\tlearn: 0.4203528\ttest: 0.4952758\tbest: 0.4952758 (185)\ttotal: 9m 20s\tremaining: 40m 52s\n",
      "186:\tlearn: 0.4202774\ttest: 0.4953460\tbest: 0.4952758 (185)\ttotal: 9m 21s\tremaining: 40m 41s\n",
      "187:\tlearn: 0.4201984\ttest: 0.4953319\tbest: 0.4952758 (185)\ttotal: 9m 22s\tremaining: 40m 29s\n",
      "188:\tlearn: 0.4201622\ttest: 0.4953006\tbest: 0.4952758 (185)\ttotal: 9m 24s\tremaining: 40m 23s\n",
      "189:\tlearn: 0.4200787\ttest: 0.4953327\tbest: 0.4952758 (185)\ttotal: 9m 25s\tremaining: 40m 12s\n",
      "190:\tlearn: 0.4200442\ttest: 0.4953174\tbest: 0.4952758 (185)\ttotal: 9m 26s\tremaining: 40m 1s\n",
      "191:\tlearn: 0.4200153\ttest: 0.4952949\tbest: 0.4952758 (185)\ttotal: 9m 27s\tremaining: 39m 49s\n",
      "192:\tlearn: 0.4199222\ttest: 0.4952579\tbest: 0.4952579 (192)\ttotal: 9m 29s\tremaining: 39m 40s\n",
      "193:\tlearn: 0.4198671\ttest: 0.4952467\tbest: 0.4952467 (193)\ttotal: 9m 30s\tremaining: 39m 29s\n",
      "194:\tlearn: 0.4198346\ttest: 0.4952279\tbest: 0.4952279 (194)\ttotal: 9m 31s\tremaining: 39m 17s\n",
      "195:\tlearn: 0.4197889\ttest: 0.4952347\tbest: 0.4952279 (194)\ttotal: 9m 31s\tremaining: 39m 6s\n",
      "196:\tlearn: 0.4197201\ttest: 0.4952152\tbest: 0.4952152 (196)\ttotal: 9m 33s\tremaining: 38m 56s\n",
      "197:\tlearn: 0.4196656\ttest: 0.4949967\tbest: 0.4949967 (197)\ttotal: 9m 34s\tremaining: 38m 46s\n",
      "198:\tlearn: 0.4195893\ttest: 0.4949804\tbest: 0.4949804 (198)\ttotal: 9m 35s\tremaining: 38m 34s\n",
      "199:\tlearn: 0.4195540\ttest: 0.4949828\tbest: 0.4949804 (198)\ttotal: 9m 35s\tremaining: 38m 23s\n",
      "200:\tlearn: 0.4194953\ttest: 0.4949575\tbest: 0.4949575 (200)\ttotal: 9m 36s\tremaining: 38m 13s\n",
      "201:\tlearn: 0.4194553\ttest: 0.4949596\tbest: 0.4949575 (200)\ttotal: 9m 37s\tremaining: 38m 3s\n",
      "202:\tlearn: 0.4193880\ttest: 0.4949353\tbest: 0.4949353 (202)\ttotal: 9m 39s\tremaining: 37m 53s\n",
      "203:\tlearn: 0.4193453\ttest: 0.4949146\tbest: 0.4949146 (203)\ttotal: 9m 40s\tremaining: 37m 43s\n",
      "204:\tlearn: 0.4193306\ttest: 0.4949588\tbest: 0.4949146 (203)\ttotal: 9m 44s\tremaining: 37m 45s\n",
      "205:\tlearn: 0.4192768\ttest: 0.4949510\tbest: 0.4949146 (203)\ttotal: 9m 45s\tremaining: 37m 36s\n",
      "206:\tlearn: 0.4192436\ttest: 0.4949472\tbest: 0.4949146 (203)\ttotal: 9m 46s\tremaining: 37m 26s\n",
      "207:\tlearn: 0.4191831\ttest: 0.4949326\tbest: 0.4949146 (203)\ttotal: 9m 47s\tremaining: 37m 17s\n",
      "208:\tlearn: 0.4191027\ttest: 0.4949418\tbest: 0.4949146 (203)\ttotal: 9m 48s\tremaining: 37m 7s\n",
      "209:\tlearn: 0.4190716\ttest: 0.4949494\tbest: 0.4949146 (203)\ttotal: 9m 49s\tremaining: 36m 58s\n",
      "210:\tlearn: 0.4190444\ttest: 0.4949663\tbest: 0.4949146 (203)\ttotal: 9m 50s\tremaining: 36m 49s\n",
      "211:\tlearn: 0.4189728\ttest: 0.4949541\tbest: 0.4949146 (203)\ttotal: 9m 54s\tremaining: 36m 47s\n",
      "212:\tlearn: 0.4189080\ttest: 0.4950128\tbest: 0.4949146 (203)\ttotal: 9m 55s\tremaining: 36m 39s\n",
      "213:\tlearn: 0.4188237\ttest: 0.4949772\tbest: 0.4949146 (203)\ttotal: 9m 56s\tremaining: 36m 30s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.4949146004\n",
      "bestIteration = 203\n",
      "\n",
      "Shrink model to first 204 iterations.\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_avg_logloss\", study.best_value)\n",
    "\n",
    "    model = cb.CatBoostClassifier(**study.best_params)\n",
    "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=10)\n",
    "    y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    mlflow.log_metric('log_loss', log_loss(y_valid, y_pred))\n",
    "    mlflow.log_metric('avg_pr', average_precision_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
