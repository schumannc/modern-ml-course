{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367a8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcus.silva/Code/modern-ml/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from week_1.notebooks.map_jobs_to_categories import map_jobs_to_categories\n",
    "from global_code.util import reduce_mem_usage\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "read-data",
   "metadata": {},
   "source": [
    "### Read the parquet file located at `./week_1/data/processed/full_churn_data_with_target.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5286530 entries, 0 to 5286529\n",
      "Data columns (total 31 columns):\n",
      " #   Column                    Dtype         \n",
      "---  ------                    -----         \n",
      " 0   Id                        int64         \n",
      " 1   customer_id               int64         \n",
      " 2   interest_rate             float64       \n",
      " 3   name                      object        \n",
      " 4   country                   object        \n",
      " 5   date_of_birth             object        \n",
      " 6   address                   object        \n",
      " 7   date                      datetime64[ns]\n",
      " 8   atm_transfer_in           int64         \n",
      " 9   atm_transfer_out          int64         \n",
      " 10  bank_transfer_in          int64         \n",
      " 11  bank_transfer_out         int64         \n",
      " 12  crypto_in                 int64         \n",
      " 13  crypto_out                int64         \n",
      " 14  bank_transfer_in_volume   float64       \n",
      " 15  bank_transfer_out_volume  float64       \n",
      " 16  crypto_in_volume          float64       \n",
      " 17  crypto_out_volume         float64       \n",
      " 18  complaints                int64         \n",
      " 19  touchpoints               object        \n",
      " 20  csat_scores               object        \n",
      " 21  tenure                    int64         \n",
      " 22  from_competitor           bool          \n",
      " 23  job                       object        \n",
      " 24  churn_due_to_fraud        bool          \n",
      " 25  model_predicted_fraud     bool          \n",
      " 26  Usage                     object        \n",
      " 27  churn                     int64         \n",
      " 28  next_date                 datetime64[ns]\n",
      " 29  days_diff                 float64       \n",
      " 30  split                     object        \n",
      "dtypes: bool(3), datetime64[ns](2), float64(6), int64(11), object(9)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('./week_1/data/processed/full_churn_data_with_target.parquet')\n",
    "df.reset_index(drop=False, inplace=True) \n",
    "df.sort_values(by=['customer_id', 'date'], ascending=True, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65bce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1144.45 MB\n",
      "Memory usage after optimization is: 675.58 MB\n",
      "Decreased by 41.0%\n"
     ]
    }
   ],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402657cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Touchpoints nested list to individual count columns\n",
    "\n",
    "# 1. Explode the touchpoints list column to transform each channel into its own row\n",
    "exploded_df = df.explode('touchpoints')\n",
    "\n",
    "# 2. Count the occurrences of each channel per original row\n",
    "counts = (exploded_df.groupby([exploded_df.index, 'touchpoints'])\n",
    "                     .size()\n",
    "                     .unstack(fill_value=0))\n",
    "\n",
    "exploded_df = None\n",
    "\n",
    "# 3. Ensure all possible columns exist (email, appointment, phone, whatsapp)\n",
    "for col in ['email', 'appointment', 'phone', 'whatsapp']:\n",
    "    if col not in counts.columns:\n",
    "        counts[col] = 0\n",
    "\n",
    "# 4. Merge these counts back into the original DataFrame\n",
    "df = df.join(counts, how='left').fillna(0)\n",
    "counts = None\n",
    "\n",
    "# 5. Convert counts to integer type\n",
    "df[['email', 'appointment', 'phone', 'whatsapp']] = (\n",
    "    df[['email', 'appointment', 'phone', 'whatsapp']].astype(int)\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "base-churn",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_between'] = df.groupby('customer_id')['date'].diff().dt.days.fillna(0)\n",
    "df['customer_age'] = (df['date'] - pd.to_datetime(df['date_of_birth'])).dt.days / 365.25\n",
    "df['from_competitor'] = df['from_competitor'].astype(int)\n",
    "df['churn_due_to_fraud'] = df['churn_due_to_fraud'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb765126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jobs to categories\n",
    "map_jobs_to_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbaf8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding window features for 7D ...\n",
      "Memory usage of dataframe is 757.18 MB\n",
      "Memory usage after optimization is: 253.02 MB\n",
      "Decreased by 66.6%\n",
      "Adding window features for 10D ...\n",
      "Memory usage of dataframe is 757.18 MB\n",
      "Memory usage after optimization is: 253.02 MB\n",
      "Decreased by 66.6%\n",
      "Adding window features for 90D ...\n",
      "Memory usage of dataframe is 757.18 MB\n",
      "Memory usage after optimization is: 253.02 MB\n",
      "Decreased by 66.6%\n",
      "Adding window features for 180D ...\n",
      "Memory usage of dataframe is 757.18 MB\n",
      "Memory usage after optimization is: 253.02 MB\n",
      "Decreased by 66.6%\n",
      "Adding window features for 365D ...\n",
      "Memory usage of dataframe is 757.18 MB\n",
      "Memory usage after optimization is: 253.02 MB\n",
      "Decreased by 66.6%\n",
      "Adding window features for 450D ...\n",
      "Memory usage of dataframe is 757.18 MB\n",
      "Memory usage after optimization is: 253.02 MB\n",
      "Decreased by 66.6%\n"
     ]
    }
   ],
   "source": [
    "def add_window_features(df, window_size=''):\n",
    "    print('Adding window features for', window_size, '...')\n",
    "    # Define the aggregation dictionary for rolling operations\n",
    "    agg_dict = {\n",
    "        'date': 'count',\n",
    "        'days_between': ['sum', 'mean', 'std', 'max', 'min'],\n",
    "        'bank_transfer_in': 'mean',\n",
    "        'bank_transfer_out': 'mean',\n",
    "        'crypto_in': 'mean',\n",
    "        'crypto_out': 'mean',\n",
    "        'bank_transfer_in_volume': ['mean', 'sum'],\n",
    "        'bank_transfer_out_volume': ['mean', 'sum'],\n",
    "        'crypto_in_volume': ['mean', 'sum'],\n",
    "        'crypto_out_volume': ['mean', 'sum']\n",
    "    }\n",
    "    \n",
    "    # Perform the groupby and rolling aggregation\n",
    "    result = (\n",
    "        df.groupby('customer_id')\n",
    "          .rolling(window_size, on='date')\n",
    "          .agg(agg_dict)\n",
    "    )\n",
    "    \n",
    "    # Flatten the MultiIndex columns and rename them\n",
    "    result.columns = [\n",
    "        f\"prior_{window_size}_{stat}_{col}\" if stat != '' else f\"prior_{window_size}_{col}\"\n",
    "        for col, stat in result.columns\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Reduce memory\n",
    "    result = reduce_mem_usage(result)\n",
    "\n",
    "    # Reset index if needed (depending on your use case)\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    # Merge back with the original DataFrame if required\n",
    "    df = df.merge(result, on=['customer_id', 'date'], how='left')\n",
    "\n",
    "    # Interations features\n",
    "    df[f'prior_{window_size}_bank_volume'] = df[f'prior_{window_size}_sum_bank_transfer_in_volume']- df[f'prior_{window_size}_sum_bank_transfer_out_volume']\n",
    "    df[f'prior_{window_size}_crypto_volume'] = df[f'prior_{window_size}_sum_crypto_in_volume']  - df[f'prior_{window_size}_sum_crypto_out_volume']\n",
    "    \n",
    "    df[f'prior_{window_size}_bank_balance'] = (\n",
    "        df[f'prior_{window_size}_bank_volume'] + df[f'prior_{window_size}_crypto_volume']\n",
    "    )\n",
    "\n",
    "    df[f'prior_{window_size}_mean_balance'] = (\n",
    "        (df[f'prior_{window_size}_mean_bank_transfer_in_volume'] - df[f'prior_{window_size}_mean_bank_transfer_out_volume'])\n",
    "        + (df[f'prior_{window_size}_mean_crypto_in_volume'] - df[f'prior_{window_size}_mean_crypto_out_volume'])\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Last 7 days\n",
    "df = add_window_features(df, '7D')\n",
    "\n",
    "# Last 10 days\n",
    "df = add_window_features(df, '10D')\n",
    "\n",
    "# Last 90 days\n",
    "df = add_window_features(df, '90D')\n",
    "\n",
    "# Last 180 days\n",
    "df = add_window_features(df, '180D')\n",
    "\n",
    "# Last 365 days\n",
    "df = add_window_features(df, '365D')\n",
    "\n",
    "# Last 365 days\n",
    "df = add_window_features(df, '450D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dade78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "window-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifetime Window features...\n"
     ]
    }
   ],
   "source": [
    "# Lifetime Window features\n",
    "print('Lifetime Window features...')\n",
    "df['prior_emails'] = df.groupby('customer_id')['email'].cumsum().values\n",
    "df['prior_appointments'] = df.groupby('customer_id')['appointment'].cumsum().values\n",
    "df['prior_phones'] = df.groupby('customer_id')['phone'].cumsum().values\n",
    "df['prior_whatsapps'] = df.groupby('customer_id')['whatsapp'].cumsum().values\n",
    "\n",
    "df['prior_touchpoints'] = df['prior_emails'] + df['prior_appointments'] + df['prior_phones'] + df['prior_whatsapps']\n",
    "df['prior_count'] = df.groupby('customer_id').cumcount() + 1\n",
    "\n",
    "df['prior_sum_days_between'] = df.groupby('customer_id')['days_between'].expanding().sum().reset_index(level=0, drop=True)\n",
    "df['prior_std_days_between'] = df.groupby('customer_id')['days_between'].expanding().std().reset_index(level=0, drop=True)\n",
    "df['prior_mean_days_between'] = df.groupby('customer_id')['days_between'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_min_days_between'] = df.groupby('customer_id')['days_between'].expanding().min().reset_index(level=0, drop=True)\n",
    "df['prior_max_days_between'] = df.groupby('customer_id')['days_between'].expanding().max().reset_index(level=0, drop=True)\n",
    "\n",
    "# Transfers and volumes\n",
    "df['prior_mean_bank_transfer_in'] = df.groupby('customer_id')['bank_transfer_in'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_mean_bank_transfer_out'] = df.groupby('customer_id')['bank_transfer_out'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_mean_crypto_in'] = df.groupby('customer_id')['crypto_in'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_mean_crypto_out'] = df.groupby('customer_id')['crypto_out'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_mean_bank_transfer_in_volume'] = df.groupby('customer_id')['bank_transfer_in_volume'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_mean_bank_transfer_out_volume'] = df.groupby('customer_id')['bank_transfer_out_volume'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_mean_crypto_in_volume'] = df.groupby('customer_id')['crypto_in_volume'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_mean_crypto_out_volume'] = df.groupby('customer_id')['crypto_out_volume'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['prior_sum_bank_transfer_in_volume'] = df.groupby('customer_id')['bank_transfer_in_volume'].cumsum().values\n",
    "df['prior_sum_bank_transfer_out_volume'] = df.groupby('customer_id')['bank_transfer_out_volume'].cumsum().values\n",
    "df['prior_sum_crypto_in_volume'] = df.groupby('customer_id')['crypto_in_volume'].cumsum().values\n",
    "df['prior_sum_crypto_out_volume'] = df.groupby('customer_id')['crypto_out_volume'].cumsum().values\n",
    "\n",
    "df['prior_transfer_balance'] = (\n",
    "    df.prior_sum_bank_transfer_in_volume - df.prior_sum_bank_transfer_out_volume\n",
    ")\n",
    "\n",
    "df['prior_crypto_balance'] = (\n",
    "    df.prior_sum_crypto_in_volume - df.prior_sum_crypto_out_volume\n",
    ")\n",
    "\n",
    "df['prior_bank_balance'] = (\n",
    "    (df.prior_sum_bank_transfer_in_volume - df.prior_sum_bank_transfer_out_volume)\n",
    "    + (df.prior_sum_crypto_in_volume - df.prior_sum_crypto_out_volume)\n",
    ")\n",
    "\n",
    "df['prior_mean_balance'] = (\n",
    "    (df.prior_mean_bank_transfer_in_volume - df.prior_mean_bank_transfer_out_volume)\n",
    "    + (df.prior_mean_crypto_in_volume - df.prior_mean_crypto_out_volume)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e042b7",
   "metadata": {},
   "source": [
    "### Processing categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaa606d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast object cols to category\n",
    "cat_features = ['job', 'country', 'broad_job_category']\n",
    "for col in cat_features:\n",
    "    df[col] = df[col].astype('str').fillna('')\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95aef4",
   "metadata": {},
   "source": [
    "### Defining diferent targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ffbde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 365 days of inactivity\n",
    "df = df.copy()\n",
    "df['churn_365'] = 0\n",
    "df.loc[df['days_diff'] >= 365, 'churn_365'] = 1\n",
    "\n",
    "#420 days of inactivity\n",
    "df['churn_420'] = 0\n",
    "df.loc[df['days_diff'] >= 420, 'churn_420'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1de8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All customers with a last activity date over a 18 months ago (reference date 2023-12-31, cutt-off date 2022-06-01).\n",
    "\n",
    "'''\n",
    "Verify this:\n",
    "\n",
    "We're never very confident in our prediction at the point of churn.\n",
    "\n",
    "We're asking a very hard question - is this transaction the last? Rather than asking has this customer churned after 100 days of inactivity have passed.\n",
    "\n",
    "(Of the 2270378 in the train data, only 9820 are \"final transactions\", ~0.43%)\n",
    "'''\n",
    "\n",
    "churned_customers = df[df.date < '2024-01-01'].groupby('customer_id')['date'].max().reset_index()\n",
    "churned_customers = churned_customers[churned_customers.date < '2022-06-01'].copy()\n",
    "churned_customers.columns = ['customer_id', 'churn_date']\n",
    "churned_customers['churn_18_months'] = 1\n",
    "\n",
    "df = pd.merge(df, churned_customers, how='left', left_on=['customer_id', 'date'], right_on=['customer_id', 'churn_date'])\n",
    "df.churn_18_months = df.churn_18_months.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b54f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 3791.33 MB\n",
      "Memory usage after optimization is: 2767.88 MB\n",
      "Decreased by 27.0%\n"
     ]
    }
   ],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f821e8",
   "metadata": {},
   "source": [
    "### Saving the processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d7f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cols that will not be used during the traning of the model\n",
    "non_features_list = ['Id', 'customer_id','name','date_of_birth','address','date', 'job', 'touchpoints','csat_scores','Usage','churn','next_date','days_diff','split','churn_365','churn_420','churn_date','churn_18_months']\n",
    "\n",
    "# Saving the features list\n",
    "features_list = list(df.drop(columns=non_features_list).columns)\n",
    "with open('./week_1/data/processed/features_list.json', 'w') as f:\n",
    "    json.dump(features_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661540c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Id'], ascending=True, inplace=True)\n",
    "df['Usage'] = df['Usage'].replace(0, 'Public')\n",
    "df.to_parquet('./week_1/data/processed/feature_engineering_dataset_v2.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
