{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo Bank Churn Prediction\n",
    "\n",
    "[IMPORTAN! Attempt to use Copilot to build a full traning notebook for kaggle, did not work well]\n",
    "\n",
    "This notebook synthesizes all the steps from the week_1 notebooks to build a complete Kaggle notebook for submission to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score, average_precision_score, brier_score_loss\n",
    "from venn_abers import VennAbersCalibrator\n",
    "import joblib\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Merge Data\n",
    "\n",
    "Load all the Parquet raw data, including train and test datasets, and merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = ['./week_1/data/raw/train_1.parquet', './week_1/data/raw/train_2.parquet', './week_1/data/raw/train_3.parquet']\n",
    "train_df = pd.concat([pd.read_parquet(file) for file in train_files], ignore_index=True)\n",
    "test_df = pd.read_parquet('./week_1/data/raw/test.parquet')\n",
    "\n",
    "print('Train Data Info:')\n",
    "print(train_df.info())\n",
    "print('Train Data Memory Usage:', train_df.memory_usage(deep=True).sum() / 1024**2, 'MB')\n",
    "print('Train Data Date Range:', train_df['date'].min(), 'to', train_df['date'].max())\n",
    "\n",
    "print('Test Data Info:')\n",
    "print(test_df.info())\n",
    "print('Test Data Date Range:', test_df['date'].min(), 'to', test_df['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Perform feature engineering and define the target for the full load data (train + test) variable (with the 420 days definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "full_df['date_of_birth'] = pd.to_datetime(full_df['date_of_birth'])\n",
    "full_df['date'] = pd.to_datetime(full_df['date'])\n",
    "\n",
    "# Feature engineering steps\n",
    "full_df['days_between'] = full_df.groupby('customer_id')['date'].diff().dt.days.fillna(0)\n",
    "full_df['customer_age'] = (full_df['date'] - full_df['date_of_birth']).dt.days / 365.25\n",
    "full_df['from_competitor'] = full_df['from_competitor'].astype(int)\n",
    "full_df['churn_due_to_fraud'] = full_df['churn_due_to_fraud'].astype(int)\n",
    "full_df['atm_transfer_in'] = np.log1p(full_df['atm_transfer_in'])\n",
    "full_df['atm_transfer_out'] = np.log1p(full_df['atm_transfer_out'])\n",
    "\n",
    "# Aggregations\n",
    "agg_funcs = {\n",
    "    'days_between': ['count', 'mean', 'max'],\n",
    "    'bank_transfer_in': 'mean',\n",
    "    'bank_transfer_out': 'mean',\n",
    "    'crypto_in': 'mean',\n",
    "    'crypto_out': 'mean',\n",
    "    'bank_transfer_in_volume': 'mean',\n",
    "    'bank_transfer_out_volume': 'mean',\n",
    "    'crypto_in_volume': 'mean',\n",
    "    'crypto_out_volume': 'mean'\n",
    "}\n",
    "\n",
    "agg_df = full_df.groupby('customer_id').agg(agg_funcs)\n",
    "agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]\n",
    "full_df = full_df.merge(agg_df, on='customer_id', how='left')\n",
    "\n",
    "# Define target\n",
    "full_df['churn'] = (full_df['date'] < '2022-10-01').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "Split the data into `train_df` (to perform training, tuning, and calibration) and `test_df` (to predict and save the prediction for submission to the competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = full_df[full_df['date'] < '2022-10-01']\n",
    "test_df = full_df[full_df['date'] >= '2022-10-01']\n",
    "\n",
    "X_train = train_df.drop(columns=['churn', 'customer_id', 'date_of_birth', 'date', 'name', 'address', 'touchpoints', 'csat_scores', 'Usage', 'next_date', 'split'])\n",
    "y_train = train_df['churn']\n",
    "\n",
    "X_test = test_df.drop(columns=['churn', 'customer_id', 'date_of_birth', 'date', 'name', 'address', 'touchpoints', 'csat_scores', 'Usage', 'next_date', 'split'])\n",
    "y_test = test_df['churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune CatBoost Model\n",
    "\n",
    "Tune the CatBoost model using all the training data with expanding window cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'random_strength': trial.suggest_loguniform('random_strength', 1e-4, 1e-1),\n",
    "        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 1e-4, 1e-1),\n",
    "        'cat_features': ['country', 'broad_job_category'],\n",
    "        'verbose': 0\n",
    "    }\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    return log_loss(y_test, y_pred)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Final Model with Venn Abers Calibration\n",
    "\n",
    "Train a final CatBoost model with the tuned parameters using Venn Abers CVAP (Cross Venn Abers Calibration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = CatBoostClassifier(**best_params)\n",
    "model_best.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "\n",
    "va = VennAbersCalibrator(estimator=model_best, inductive=False, n_splits=2)\n",
    "va.fit(X_train, y_train)\n",
    "va_cv_prob = va.predict_proba(X_test)\n",
    "\n",
    "logloss_best = log_loss(y_test, va_cv_prob[:, 1])\n",
    "roc_auc_best = roc_auc_score(y_test, va_cv_prob[:, 1])\n",
    "avg_precision_best = average_precision_score(y_test, va_cv_prob[:, 1])\n",
    "brier_best = brier_score_loss(y_test, va_cv_prob[:, 1])\n",
    "\n",
    "print(f'Log Loss: {logloss_best}')\n",
    "print(f'ROC AUC: {roc_auc_best}')\n",
    "print(f'Average Precision: {avg_precision_best}')\n",
    "print(f'Brier Score: {brier_best}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Prediction\n",
    "\n",
    "Save the prediction from the test dataset for submission to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'customer_id': test_df['customer_id'], 'churn_probability': va_cv_prob[:, 1]})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission saved to submission.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
